{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data table management & advanced pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains further processing steps of the data output of `manage_extraction_output.ipynb`\n",
    "* Imputing missing values (NA values => mean column value or zero)\n",
    "* Standardizing values in numeric columns (3 different standardizers)\n",
    "* Consolidating data from all recordings and dividing up features and labels\n",
    "* Exporting to NumPy arrays\n",
    "\n",
    "The NumPy arrays then serve as the input to the combined frame-word neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from sklearn import preprocessing, impute\n",
    "from promdetect.prep import process_annotations\n",
    "from functools import reduce\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ordered list of recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_RECS = []\n",
    "with open(\"/home/lukas/Dokumente/Uni/ma_thesis/promdetect/data/dirndl/list_recordings.txt\") as recordings:\n",
    "    for recording in recordings:\n",
    "        LIST_RECS.append(recording.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values (using record-internal mean value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(df):\n",
    "    df_measurements = df[[\"dur_normed\", \"int_rms\",\n",
    "                  \"int_min\", \"int_max\", \"int_mean\",\n",
    "                  \"int_std\", \"int_min_pos\", \"int_max_pos\",\n",
    "                  \"f0_min\", \"f0_max\", \"f0_mean\", \"f0_std\",\n",
    "                  \"f0_slope\", \"f0_exc_ip\", \"f0_exc_utt\",\n",
    "                  \"f0_min_pos\", \"f0_max_pos\", \"tilt_min\",\n",
    "                  \"tilt_max\", \"tilt_mean\", \"tilt_range\", \n",
    "                  \"cog\", \"h1_h2\"]]\n",
    "    \n",
    "    imputer = impute.SimpleImputer()\n",
    "    df_imp = pd.DataFrame(imputer.fit_transform(df_measurements))\n",
    "    df_imp.columns = df_measurements.columns\n",
    "    df_imp.index= df_measurements.index\n",
    "    \n",
    "    df_imp[[\"has_accent\", \"label\", \"start\", \"end\"]] = df[[\"has_accent\", \"label\", \"start\", \"end\"]]\n",
    "    \n",
    "    return df_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize values using sklearn.preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    df_standard = df.copy()\n",
    "    abs_scaler = preprocessing.MinMaxScaler()\n",
    "    neg_pos_scaler = preprocessing.MinMaxScaler(feature_range=[-1,1])\n",
    "    robust_scaler = preprocessing.RobustScaler()\n",
    "    \n",
    "    cols_abs = [\"dur_normed\", \"int_rms\", \"int_min\", \"int_max\", \"int_mean\",\n",
    "               \"int_std\", \"f0_min\", \"f0_max\", \"f0_mean\", \"f0_exc_ip\",\n",
    "               \"f0_exc_utt\", \"tilt_max\", \"tilt_mean\", \"tilt_range\", \"cog\"]\n",
    "    cols_neg_pos = [\"tilt_min\"]\n",
    "    cols_robust = [\"f0_std\", \"f0_slope\", \"h1_h2\"]\n",
    "    \n",
    "    df_standard[cols_abs] = abs_scaler.fit_transform(df_standard[cols_abs].values)\n",
    "    df_standard[cols_neg_pos] = neg_pos_scaler.fit_transform(df_standard[cols_neg_pos].values)\n",
    "    df_standard[cols_robust] = robust_scaler.fit_transform(df_standard[cols_robust].values)\n",
    "    \n",
    "    return df_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all but one element for each series of sentence delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dup_p(df):\n",
    "    for i, g in df.groupby([(df.label != df.label.shift()).cumsum()]):\n",
    "        if (g[\"label\"].any() == \"<P>\"):\n",
    "            df.drop(df.loc[df.index.isin(g.index[:-1])].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the functions defined above and consolidate data into a unified array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/lukas/Dokumente/Uni/ma_thesis/promdetect/data/features/word_based/sets/main.pickle\", \"rb\") as file:\n",
    "    main = pickle.load(file)\n",
    "\n",
    "data = []\n",
    "for recording in LIST_RECS:\n",
    "    df = main[recording]\n",
    "    \n",
    "    df_imp = impute_missing(df)\n",
    "    \n",
    "    df_standard = standardize(df_imp)    \n",
    "    \n",
    "    df_standard[\"has_accent\"] = np.where(df_standard[\"has_accent\"].notna(), 1, 0)\n",
    "    \n",
    "    drop_dup_p(df_standard)\n",
    "    \n",
    "    arr = df_standard.to_numpy()\n",
    "    \n",
    "    arr = arr[~pd.isnull(arr).any(axis=1)]\n",
    "    \n",
    "    data.append(arr) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = np.concatenate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape array to sentence-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts_all = np.split(data_all, np.where(data_all[:, -3] == \"<P>\")[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate labels from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for utt in utts_all:\n",
    "    if len(utt) > 1:\n",
    "        utt_cleaned = np.delete(utt, np.s_[0], 0)\n",
    "    else:\n",
    "        utt_cleaned = utt\n",
    "    labels.append(utt_cleaned[:, -4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary columns from the feature array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts_all_cleaned = []\n",
    "for utt in utts_all:\n",
    "    utt_cleaned = np.delete(utt, np.s_[-4:], 1)\n",
    "    if len(utt_cleaned) > 1:\n",
    "        utt_cleaned = np.delete(utt_cleaned, np.s_[0], 0)\n",
    "    utts_all_cleaned.append(utt_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store as NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../data/features/word_based/sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(\"word_features.npy\", np.array(utts_all_cleaned, dtype=object))\n",
    "np.save(\"word_labels.npy\", np.array(labels, dtype=object))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('promdetect': venv)",
   "language": "python",
   "name": "python37364bitpromdetectvenvf0d8aec3b1634575ba564862f6a926be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "2000"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
