{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data table management & preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to manage the output of the extraction scripts `extract_features.py`, `find_syllable_nuclei.py` and `prepare_data.py`.\n",
    "* Cleans out duplicates found by the syllable nucleus detection algorithm\n",
    "* Imputes missing data by replacing NAs with mean feature values\n",
    "* Standardizes data with 3 different standardizers\n",
    "* Consolidates DataFrames\n",
    "* (optional: Balances classes using SMOTENC)\n",
    "* Reshapes DataFrames to sequences of nuclei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn import preprocessing, impute, decomposition\n",
    "from promdetect.prep import process_annotations\n",
    "from functools import reduce\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import stored feature DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recordings: 55\n"
     ]
    }
   ],
   "source": [
    "features_dir = \"../../data/features/nucleus_based/\"\n",
    "os.chdir(features_dir)\n",
    "recordings = glob(\"raw/dlf*\")\n",
    "print(\"Number of recordings:\", len(recordings)) # should be 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pre-processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and clean out duplicates within each recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reformat(recording):\n",
    "    df = pd.read_csv(recording)\n",
    "    df[\"recording\"] = recording\n",
    "    speaker_info = process_annotations.AnnotationReader(recording).get_speaker_info()\n",
    "    df[\"speaker_gender\"] = speaker_info[1]\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['start_est', 'end']).copy()\n",
    "    df[\"has_accent\"] = np.where(df[\"accent_label\"].notna(), 1, 0)\n",
    "    df[\"gender\"] = np.where(df[\"speaker_gender\"] == \"m\", 1, 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Impute missing values (using recording-internal mean value for the feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(df):\n",
    "    # select all columns except for metadata columns\n",
    "    df_measurements = df.copy().drop([\"recording\", \"speaker_gender\", \"accent_label\", \"Unnamed: 0\", \"index\", \"nucl_time\",\n",
    "                                \"phone\", \"word\", 'bound_tone', 'start_est',\n",
    "                                 'end', 'word_start', 'word_end', 'ip_start',\n",
    "                                 'ip_end', 'accent_time', 'accent_label'], axis=1)\n",
    "    imputer = impute.SimpleImputer()\n",
    "    df_imp = pd.DataFrame(imputer.fit_transform(df_measurements))\n",
    "    df_imp.columns = df_measurements.columns\n",
    "    df_imp.index= df_measurements.index\n",
    "    \n",
    "    df_imp[[\"gender\", \"has_accent\"]] = df[[\"gender\", \"has_accent\"]]\n",
    "    \n",
    "    return df_imp    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize values using sklearn.preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    df_standard = df.copy()\n",
    "    abs_scaler = preprocessing.MinMaxScaler()\n",
    "    neg_pos_scaler = preprocessing.MinMaxScaler(feature_range=[-1,1])\n",
    "    robust_scaler = preprocessing.RobustScaler()\n",
    "    \n",
    "    cols_abs = [\"duration_est\", \"rms\", \"duration_normed\", \"min_intensity_nuclei\", \"max_intensity_nuclei\", \"intensity_std_nuclei\", \"mean_intensity_nuclei\", \"min_intensity_pos\", \"max_intensity_pos\", \"f0_max_nuclei\", \"f0_min_nuclei\", \"f0_mean_nuclei\", \"f0_range_nuclei\", \"f0_std_nuclei\", \"f0_min_pos\", \"f0_max_pos\"]\n",
    "    cols_neg_pos = [\"excursion_word\", \"excursion_ip\"]\n",
    "    cols_robust = [\"pitch_slope\", \"f0_range_nuclei\", \"spectral_tilt_mean\", \"spectral_tilt_range\", \"min_spectral_tilt\", \"max_spectral_tilt\", \"spectral_cog\", \"h1_h2\"]\n",
    "    \n",
    "    df_standard[cols_abs] = abs_scaler.fit_transform(df_standard[cols_abs].values)\n",
    "    df_standard[cols_neg_pos] = neg_pos_scaler.fit_transform(df_standard[cols_neg_pos].values)\n",
    "    df_standard[cols_robust] = robust_scaler.fit_transform(df_standard[cols_robust].values)\n",
    "    \n",
    "    df_standard = df_standard.drop(columns=\"has_accent\")\n",
    "    \n",
    "    return df_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Run the functions defined above and consolidate data into unified arrays, separate for features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without SMOTENC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for recording in recordings:\n",
    "    \n",
    "    df = read_reformat(recording)\n",
    "    \n",
    "    df_imp = impute_missing(df)\n",
    "    \n",
    "    df_standard = standardize(df_imp)\n",
    "    \n",
    "    data.append(df_standard.to_numpy())\n",
    "    labels.append(df_imp[\"has_accent\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels_np = np.array(labels, dtype=object)\n",
    "data_np = np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the two to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sets/data_standard.npy\", data_np)\n",
    "np.save(\"sets/data_standard_labels.npy\", labels_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_smotenc = []\n",
    "labels_smotenc = []\n",
    "for recording in recordings:\n",
    "    \n",
    "    df = read_reformat(recording)\n",
    "    \n",
    "    df_imp = impute_missing(df)\n",
    "    \n",
    "    df_standard = standardize(df_imp)\n",
    "\n",
    "    smote = SMOTENC(categorical_features=[20])\n",
    "    data_balanced, labels_balanced = smote.fit_resample(df_standard.to_numpy(), df_imp[\"has_accent\"].to_numpy())\n",
    "    \n",
    "    data_smotenc.append(data_balanced)\n",
    "    labels_smotenc.append(labels_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np_smotenc = np.array(labels_smotenc, dtype=object)\n",
    "data_np_smotenc = np.array(data_smotenc, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sets/data_standard_smotenc.npy\", data_np_smotenc)\n",
    "np.save(\"sets/data_standard_labels_smotenc.npy\", labels_np_smotenc)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('promdetect': venv)",
   "language": "python",
   "name": "python37364bitpromdetectvenvf0d8aec3b1634575ba564862f6a926be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "2000"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
