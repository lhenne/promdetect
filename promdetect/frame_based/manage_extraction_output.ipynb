{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame-level set data management script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is used to clean up the data extracted by the `extract_frame_features.py` and `prep_data.py` scripts.\n",
    "* Invalid feature values are corrected, additional information in the form of accent labels and start timestamps are added to the main feature DataFrame for each recording.\n",
    "* Word labels that are not lexical words or sentence delimiters are removed from the DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from promdetect.prep import process_annotations\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process accent and word label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_files = glob(\"/home/lukas/Dokumente/Uni/ma_thesis/quelldaten/DIRNDL-prosody/*.accents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for file in label_files:\n",
    "    df = process_annotations.AnnotationReader(file).get_annotation_data()\n",
    "    recording_id = str(Path(file).stem)\n",
    "    labels[recording_id] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_files = glob(\"/home/lukas/Dokumente/Uni/ma_thesis/quelldaten/DIRNDL-prosody/*.words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "for file in word_files:\n",
    "    df = process_annotations.AnnotationReader(file).get_annotation_data()\n",
    "    recording_id = str(Path(file).stem)\n",
    "    words[recording_id] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect frame-level feature DataFrames and pipe each through a number of preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recordings: 55\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/lukas/Dokumente/Uni/ma_thesis/promdetect/data/features/frame_based\")\n",
    "recordings = glob(\"raw/dlf*\")\n",
    "print(\"Number of recordings:\", len(recordings)) # should be 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing recording 55\r"
     ]
    }
   ],
   "source": [
    "data = {}  # Dictionary in which all the processed data will be stored (recording ID => data)\n",
    "idx = 0\n",
    "NONWORD_TYPES = [\"[@]\", \"[t]\", \"[n]\", \"[f]\", \"[h]\", np.nan] # Set types of word labels to be dropped\n",
    "\n",
    "for recording in recordings:\n",
    "    idx += 1\n",
    "    recording_id = str(Path(Path(recording).stem).stem)\n",
    "    df = pd.read_csv(recording)\n",
    "    \n",
    "    # Harmonics-to-noise ratio of -200.0 means that it could not be determined\n",
    "    df.loc[df[\"hnr\"] == -200.0, \"hnr\"] = np.nan\n",
    "    # Pitch above 500 Hz is outside of the pitch contour limits, so it is reduced to NaN\n",
    "    df.loc[df[\"f0\"] > 500, \"f0\"] = np.nan\n",
    "    df = df.drop(columns=\"Unnamed: 0\")\n",
    "    \n",
    "    # Prepare columns for labels\n",
    "    df[\"word\"] = np.nan\n",
    "    df[\"accent\"] = np.nan\n",
    "    \n",
    "    # Locate recording in dictionary of word DataFrames, \n",
    "    # then locate current word containing correct frame timestamps\n",
    "    words_df = words[recording_id]\n",
    "    for row in words_df.itertuples():\n",
    "        df.loc[(row.start_est <= df[\"time\"]) & (df[\"time\"] <= row.end), \"word\"] = row.label\n",
    "\n",
    "    # Locate recording in dictionary of accent label DataFrames, \n",
    "    # then locate label within correct frame timestamps\n",
    "    labels_df = labels[recording_id]\n",
    "    for row in labels_df.itertuples():\n",
    "        df.loc[(df[\"time\"] <= row.time) & (row.time <= df[\"time\"] + 0.01), \"accent\"] = row.label\n",
    "    \n",
    "    # Turn ToBI accent labels into binary labels for 'has_accent' column\n",
    "    df[\"has_accent\"] = np.nan\n",
    "    for i, g in df.groupby([(df.word != df.word.shift()).cumsum()]):\n",
    "        if (g[\"accent\"].notna()).any():\n",
    "            df.loc[g.index, \"has_accent\"] = True\n",
    "            \n",
    "    # Filter out frames within the boundaries of nonword labels \n",
    "    df_words_delimiters = df.loc[~df[\"word\"].isin(NONWORD_TYPES)]\n",
    "    \n",
    "    data[recording_id] = df_words_delimiters\n",
    "    \n",
    "    print(\"Finished processing recording\", idx, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the cleaned data set in a single .pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sets/main.pickle\", \"wb\") as file:\n",
    "    pickle.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('promdetect': venv)",
   "language": "python",
   "name": "python37364bitpromdetectvenvf0d8aec3b1634575ba564862f6a926be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "2000"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
