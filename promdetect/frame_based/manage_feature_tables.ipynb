{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data table management & advanced preprocessing steps\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains utilities to further pre-process the frame-level feature data with the following steps:\n",
    "* Imputing missing values (NA values => mean column value or zero)\n",
    "* Standardizing values in numeric columns (3 different standardizers)\n",
    "* Consolidating data from all recording and dividing up features and labels\n",
    "    * Reshaping into sentence-level frame sequences\n",
    "* Exporting to NumPy arrays\n",
    "\n",
    "The NumPy arrays then serve as the input to the frame-level neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from sklearn import preprocessing, impute\n",
    "from promdetect.prep import process_annotations\n",
    "from functools import reduce\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ordered list of recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_RECS = []\n",
    "with open(\"/home/lukas/Dokumente/Uni/ma_thesis/promdetect/data/dirndl/list_recordings.txt\") as recordings:\n",
    "    for recording in recordings:\n",
    "        LIST_RECS.append(recording.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read feature data (output of `manage_extraction_output.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/lukas/Dokumente/Uni/ma_thesis/promdetect/data/features/frame_based/sets\")\n",
    "with open(\"main.pickle\", \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values (using recording-internal mean value for the feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(df):\n",
    "    df_measurements = df[[\"f0\", \"voicing_pr\",\n",
    "                  \"rms\", \"loudness\", \"zcr\",\n",
    "                  \"hnr\"]]\n",
    "    \n",
    "    imputer = impute.SimpleImputer()\n",
    "    df_imp = pd.DataFrame(imputer.fit_transform(df_measurements))\n",
    "    df_imp.columns = df_measurements.columns\n",
    "    df_imp.index= df_measurements.index\n",
    "    \n",
    "    df_imp[[\"time\", \"word\", \"accent\", \"has_accent\"]] = df[[\"time\", \"word\", \"accent\", \"has_accent\"]]\n",
    "    \n",
    "    return df_imp    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize values using sklearn.preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    df_standard = df.copy()\n",
    "    abs_scaler = preprocessing.MinMaxScaler()\n",
    "    neg_pos_scaler = preprocessing.MinMaxScaler(feature_range=[-1,1])\n",
    "    \n",
    "    cols_abs = [\"f0\", \"voicing_pr\", \"rms\", \"loudness\", \"zcr\"]\n",
    "    cols_neg_pos = [\"hnr\"]\n",
    "    \n",
    "    df_standard[cols_abs] = abs_scaler.fit_transform(df_standard[cols_abs].values)\n",
    "    df_standard[cols_neg_pos] = neg_pos_scaler.fit_transform(df_standard[cols_neg_pos].values)\n",
    "    \n",
    "    return df_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all but one frame for each sentence delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dup_p(df):\n",
    "    # Group into consecutive frames with the same word label\n",
    "    for i, g in df.groupby([(df.word != df.word.shift()).cumsum()]):\n",
    "        if (g[\"word\"].any() == \"<P>\"):\n",
    "            df.drop(df.loc[df.index.isin(g.index[:-1])].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Run the functions defined above and consolidate data into unified array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing recording 55\r"
     ]
    }
   ],
   "source": [
    "simple_dfs = []\n",
    "idx = 0\n",
    "\n",
    "for recording in LIST_RECS:\n",
    "    idx += 1\n",
    "    df = data[recording]\n",
    "    \n",
    "    df_imp = impute_missing(df)\n",
    "    \n",
    "    df_standard = standardize(df_imp)\n",
    "    \n",
    "    drop_dup_p(df_standard)\n",
    "    \n",
    "    arr = df_standard.to_numpy()\n",
    "    \n",
    "    simple_dfs.append(arr)\n",
    "    \n",
    "    print(\"Finished processing recording\", idx, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "simple_data = np.concatenate(simple_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape array to sentence-level (sentence x frames x features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simple_utts = np.split(simple_data, np.where(simple_data[:, -3] == \"<P>\")[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Coordinate frame timestamps with identical words (to be able to find corresponding words later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetables = []\n",
    "for utt in simple_utts:\n",
    "    timetable = []\n",
    "    df = pd.DataFrame(utt[:, -4:-2], columns=[\"time\", \"word\"])\n",
    "    # Group into consecutive frames with the same word label\n",
    "    for i, g in df.groupby([(df.word != df.word.shift()).cumsum()]):\n",
    "        start = g.index[0]\n",
    "        end = g.index[-1]\n",
    "        if end != 0:\n",
    "            timetable.append((g.index[0], g.index[-1]))\n",
    "    if not timetable:\n",
    "        timetable.append((0, 0))\n",
    "    timetable = np.array(timetable)\n",
    "    timetables.append(timetable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate labels from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_labels = []\n",
    "for utt in simple_utts:\n",
    "    accent_bin = np.where(np.isnan(utt[:, -1].astype(\"float64\")), 0, 1)\n",
    "    simple_labels.append(accent_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns from feature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_utts_cleaned = []\n",
    "for utt in simple_utts:\n",
    "    simple_utts_cleaned.append(np.delete(utt, np.s_[-3:], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store as NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(\"frame_features.npy\", np.array(simple_utts_cleaned, dtype=object))\n",
    "np.save(\"frame_labels.npy\", np.array(simple_labels, dtype=object))\n",
    "np.save(\"frame_times.npy\", np.array(timetables, dtype=object))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('promdetect': venv)",
   "language": "python",
   "name": "python37364bitpromdetectvenvf0d8aec3b1634575ba564862f6a926be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "2000"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "657.157px",
    "left": "1767.71px",
    "right": "20px",
    "top": "119.983px",
    "width": "347.763px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
